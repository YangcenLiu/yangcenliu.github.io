<!DOCTYPE html>
<html lang="en">
<head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-F76XVSKN3V"></script>
  <title>Yangcen Liu @ GaTech</title>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Yangcen Liu is a M.S. student at Georgia Tech, with research interests on machine learning for embodied agents.">
  <meta name="keywords" content="Yangcen Liu,Yangcen,Gatech,Artificial Intelligence,Computer Vision,Robotics,Machine Learning">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css" crossorigin="anonymous">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
  <link rel="shortcut icon" href="static/img/gatech.png">
  <meta property="og:image" content="images/luo7.jpg">

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link href="https://fonts.googleapis.com/css2?family=EB+Garamond:wght@400;600;700&family=Inter:wght@400;500;600&display=swap" rel="stylesheet">

  <style type="text/css">
  :root{
    --bg:#faf7f2;
    --text:#262626;
    --muted:#6b6b6b;
    --line:#eae4db;
    --link:#224e7a;
    --link-hover:#163a5b;
  }
  *{box-sizing:border-box}
  html,body{height:100%}
  body{
    margin:0;padding:0;background:var(--bg);
    color:var(--text);
    font-family: "EB Garamond", Georgia, serif;
    font-size:16px; line-height:1.85;
  }
  a{ color:var(--link); text-decoration:none; }
  a:hover{ color:var(--link-hover); text-decoration:underline; }
  .hide{display:none}

    .site-layout{
    display:grid;
    grid-template-columns: 1fr; /* ÈªòËÆ§ÊâãÊú∫Á´ØÂçïÊ†è */
    gap:32px;
    max-width:1120px;
    margin:0 auto;
    padding:24px 16px 24px;
    }

    @media (min-width:900px){
    .site-layout{
        grid-template-columns: 190px minmax(0, 3fr); /* Â§ßÂ±èÂèåÊ†è */
        gap:200px;
        padding:48px 24px 24px;
    }
    }

    @media (max-width:899px){
        .site-sidebar{
        position: static;
        top: auto;
        max-height: none;
        overflow: auto;
        margin: 0 auto;
        text-align: center;
    }
    }

    .award-badge {
        display: inline-block;
        background-color: #fff3cd;  /* light yellow like Best Paper */
        color: #856404;             /* dark gold text */
        font-size: 0.75rem;
        font-weight: 600;
        padding: 2px 6px;
        border-radius: 4px;
        white-space: nowrap;
        }


    .sidebar-content{display:flex;flex-direction:column;gap:16px}

    .avatar-wrapper{width:100%;max-width:280px;aspect-ratio:4/5;border-radius:24px;overflow:hidden;border:1px solid var(--line);background:#fff}
    .avatar-wrapper img{width:100%;height:100%;object-fit:cover;display:block}
    @media (max-width:899px){.avatar-wrapper{max-width:220px}}

    .name{font-weight:700;font-size:35px;line-height:1.08;margin:18px 0 8px}
    .role{font-size:15px;color:var(--muted);letter-spacing:.02em;text-transform:uppercase;margin-top:4px}

    .profile-links ul{list-style:none;padding:0;margin:10px 0 0;display:flex;gap:24px}
    .profile-links a{font-size:16px}

    .contact-list{list-style:none;padding:0;margin:14px 0 0}
    .contact-list li{display:flex;align-items:center;gap:12px;margin:12px 0;color:var(--muted)}
    .contact-list i{width:18px;text-align:center;color:var(--muted)}
    .contact-list a{color:var(--muted)}
    .contact-list a:hover{color:var(--link-hover)}


  .site-main{ min-width:0; }
  #main-more-container{ background:transparent; padding:0; }
  #main-bio-container{ margin:0 auto; max-width:720px; padding-top:0; font-size:16px; }
  #main-bio-container p{ margin:0 0 18px; }

  #main-pub-container{ margin:40px auto 0; max-width:900px; }
  #main-pub-container .subtitle{
    font-family:"EB Garamond",Georgia,serif; font-weight:700; font-size:28px; color:var(--text); margin:8px 0 8px;
    border-bottom:1px solid var(--line); padding-bottom:8px;
  }

  #main-pub-card-container{ width:100%; padding:0; margin:4px 0 0; }
  #main-pub-card-container>h5{ margin-bottom:16px; }
  .pub-card{
  width:100%; padding:18px 0; margin:0;
  border-top:1px solid var(--line); border-bottom:0; border-left:0; border-right:0;
  background:transparent; box-shadow:none; border-radius:0;
    }
    .pub-card .row{
    display:flex; flex-wrap:nowrap; align-items:flex-start; margin:0;
    }
    .pub-card .col-l{
    flex:0 0 240px; max-width:240px; padding-right:10px; text-align:left;
    }
    .pub-card .col-r{
    flex:1 1 0; min-width:280px; padding-left:0;
    }
    @media (max-width:992px){
    .pub-card .col-l{ flex:0 0 220px; max-width:220px; }
    }
    @media (max-width:700px){
    .pub-card .row{ flex-wrap:wrap; }
    .pub-card .col-l{ flex:0 0 100%; max-width:100%; padding-right:0; margin-bottom:0; }
    .pub-card .col-r{ flex:0 0 100%; max-width:100%; }
    }

  .pub-img{ width:100%; height:auto; border-radius:8px; object-fit:cover; background:#fff; border:1px solid var(--line); }
  .pub-card-body{ width:100%; background:transparent; padding-left:0; }
  .pub-card-body h5.title{
    font-family:"EB Garamond",Georgia,serif; font-size:20px; font-weight:600; color:var(--text); margin:0 0 6px;
  }
  .pub-card-body h6.authors{ font-size:15px; color:var(--muted); margin:0 0 6px; }
  .pub-card-body p.info{ font-size:15px; color:var(--muted); margin:0; }

  .embodiment-grid{
    display:grid; grid-template-columns:repeat(auto-fit,minmax(180px,1fr));
    gap:20px; margin-top:20px; max-width:900px; margin-left:auto; margin-right:auto;
  }
  .embodiment-card{ background:transparent; border-radius:12px; border:1px solid var(--line); overflow:hidden; }
  .embodiment-img{ width:100%; aspect-ratio:4/3; object-fit:cover; display:block; }
  .embodiment-desc{ padding:10px 12px; font-size:14px; color:var(--muted); margin:0; }

  #travel-map{ max-width:1200px; margin:56px auto 32px; padding:0 24px; }
  #travel-map .subtitle{ font-family:"EB Garamond",Georgia,serif; font-weight:700; font-size:28px; margin:0 0 12px; }

@media (min-width:900px){
  .site-layout{ align-items:start; }
  .site-layout > .site-sidebar,
  .site-layout > .site-main{ min-height:0; }

  .site-sidebar{
    position: sticky;
    top: 32px;
    max-height: calc(100vh - 32px - 30px);
    overflow: auto;
    -webkit-overflow-scrolling: touch;
    overscroll-behavior: contain;
    padding-bottom: 30px;
    scrollbar-width: none;
  }
}

@media (max-width:899px){
  .site-sidebar{
    position: static;
    top: auto;
    max-height: none;
    overflow: auto;
  }
}



</style>

</head>

<body>
  <div class="site-layout">

    <aside class="site-sidebar">
    <div class="sidebar-content">
        <div class="avatar-wrapper">
        <img src="images/blue.jpg" alt="Yangcen Liu">
        </div>

        <h1 class="name">Yangcen Liu</h1>
        <div class="role">M.S. Student ¬∑ Georgia Tech</div>


        <ul class="contact-list">
        <li><i class="fa fa-envelope-o"></i><a href="mailto:yliu3735@gatech.edu">yliu3735@gatech.edu</a></li>
        <li><i class="fa fa-graduation-cap"></i><a href="https://scholar.google.com/citations?user=Xq-r3dIAAAAJ&hl=en&oi=ao" target="_blank">Google Scholar</a></li>
        <li><i class="fa fa-twitter"></i><a href="https://x.com/Randle_Liu" target="_blank">X</a></li>
        <li><i class="fa fa-github"></i><a href="https://github.com/Randle-Github" target="_blank">GitHub</a></li>
        <li><i class="fa fa-linkedin-square"></i><a href="https://www.linkedin.com/in/yangcenliu/" target="_blank">LinkedIn</a></li>
        </ul>
    </div>
    </aside>


    <main class="site-main">

      <div id="main-more-container">
        <div id="main-bio-container">
          <p>
            <p style="color:red; font-size: 20px; font-weight: bold;">
              I am actively seeking a PhD position in Robotics&amp;CV in the USA for Fall 2026!
            </p>
            <p>
            I am a Master's student at <a href="https://www.gatech.edu/">Georgia Institute of Technology</a> (since Fall 2024), 
            in the <a href="https://rl2.cc.gatech.edu/">Robot Learning and Reasoning (RL2) Lab</a> advised by 
            <a href="https://faculty.cc.gatech.edu/~danfei/">Prof. Danfei Xu</a>. I'm also closely working with <a href="https://harishravichandar.com/">Prof. Harish Ravichandar</a>.
            </p>
            <p>
            Previously, I earned my B.Eng. from <a href="https://en.uestc.edu.cn/">University of Electronic Science and Technology of China</a> (2020‚Äì2024), 
            where I was a member of the <a href="https://diggers.ai/">Data Intelligence Group</a> advised by 
            <a href="https://wenli-vision.github.io/">Prof. Wen Li</a>.
            </p>

            <p>
            I also had the privilege of working with <a href="https://cse.buffalo.edu/~jsyuan/index.html">Prof. Junsong Yuan</a>
            during my internship at the <a href="https://www.buffalo.edu/">University at Buffalo</a>.
            </p>

            <p>
            My research lies at the intersection of robotics and computer vision:
            </p>

            <p>
            ‚Ä¢ <b>Robot Learning from Human Video:</b> Learning robot skills by imitation learning using passive human videos, finally extending to web-scale internet videos.<br>
            ‚Ä¢ <b>Dexterous Manipulation:</b> Learning human-level dexterity with combination of imitation learning and reinforcement learning. Especially for challenging humanoid bimanual dexterous manipulation tasks.<br>
            ‚Ä¢ <b>Robot Learning Systems:</b> Developing full-stack universal structure to learn from multi-modality human data, with co-design from hardware to algorithm optimization.
            </p>

            </p>
            <p>
            Outside research, I enjoy basketball, badminton, music (classical & saxophone), and traveling. 
            I'm a fan of LeBron James, Max Verstappen, and Cristiano Ronaldo.
            </p>
            </div>



        <div id="main-pub-container">
            <h2>Education</h2>
            <hr>
            <div class="pub-card">
                <div class="row">
                    <div class="col-l col-xs-12 col-lg-3">
                        <img src="data/gatech.png" width="100%" class="pub-img" alt="Georgia Tech Logo"/>
                    </div>
                    <div class="col-r col-xs-12 col-lg-9">
                        <div class="pub-card-body">
                            <h5 class="title">Georgia Institute of Technology</h5>
                            <p>Aug 2024 - June 2026 (Expected)</p>
                            <h6 class="authors">Master of Science in Robotics (College of Computing)</h6>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pub-card">
                <div class="row">
                    <div class="col-l col-xs-12 col-lg-3">
                        <img src="data/uestc_icon.png" width="100%" class="pub-img" alt="UESTC Logo"/>
                    </div>
                    <div class="col-r col-xs-12 col-lg-9">
                        <div class="pub-card-body">
                            <h5 class="title">University of Electronic Science and Technology of China</h5>
                            <p>Sep 2020 - June 2024</p>
                            <h6 class="authors">Bachelor of Engineering in Artificial Intelligence (College of Computer Science)</h6>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        
        
        <div id="main-pub-container">
            <h2>Publications & Preprints</h2>
            <p style="font-size: 14px; color: gray; margin-top: 10px;">
            <sup>*</sup> Equal contribution &nbsp;&nbsp; <sup>‚Ä†</sup> Corresponding author
            </p>

            <!-- Robotics Section -->
            <h6 class="pub-type-header">Robotics</h6>


            <div class="main-pub-card-container">
                <div class="pub-card" data-year="2025">
                    <div class="row">
                        <div class="col-l col-xs-12 col-lg-3">
                            <div class="pub-img-wrapper">
                                <img src="data/projects/EgoEngine.png" class="pub-img" />
                            </div>
                        </div>                        
                        <div class="col-r col-xs-12 col-lg-9">
                            <div class="pub-card-body">
                                <h5 class="title">EgoEngine: From Egocentric Human Videos to High-Fidelity Dexterous Robot Demonstrations</h5>
                                <h6 class="authors">
                                    Shuo Cheng<sup>*</sup>, <strong>Yangcen Liu</strong><sup>*</sup>, Yiran Yang, Xinchen Yin, Woo Chul Shin, Mengying Lin, Danfei Xu<sup>‚Ä†</sup>
                                </h6>
                                <p class="info">
                                    <span class="conference">
                                        Coming Soon! /
                                    <a href="https://egoengine.github.io/">Project Page</a> <br/> 
                                    EgoEngine is a powerful data engine that converts egocentric human videos into robot-executable demonstrations. 
                                    By coupling an action branch (retargeting with refinforcement learning refinement) and a visual branch (arm-hand inpainting with robot 
                                    rendering blending) under the same task, EgoEngine minimizes action and visual gaps to yield scalable, executable robot data. 
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="main-pub-card-container">
                <div class="pub-card" data-year="2025">
                    <div class="row">
                        <div class="col-l col-xs-12 col-lg-3">
                            <div class="pub-img-wrapper">
                                <img src="data/projects/ImMimic.png" class="pub-img" />
                            </div>
                        </div>                        
                        <div class="col-r col-xs-12 col-lg-9">
                            <div class="pub-card-body">
                                <h5 class="title">ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation</h5>
                                <h6 class="authors">
                                    <strong>Yangcen Liu</strong><sup>*</sup>, Woochul Shin<sup>*</sup>, Yunhai Han, Zhenyang Chen, Harish Ravichandar, Danfei Xu<sup>‚Ä†</sup>
                                </h6>
                                <p class="info">
                                    <span class="conference">
                                        CoRL 2025 <span class="award-badge">üèÜ Oral</span>, 
                                        RSS 2025 Dex WS <span class="award-badge">üèÜ Spotlight</span>
                                        CoRL 2025 H2R WS
                                        </span> /
                                    <a href="https://sites.google.com/view/immimic">Project Page</a> / 
                                    <a href="https://openreview.net/pdf/71eeb1380524067a776d1c563ac1a37bf0f94f98.pdf">Paper</a> / 
                                    <a href="https://www.youtube.com/watch?v=7a5HYjQ4wJo">Talk 01:24:00</a><br>
                                    ImMimic is an embodiment-agnostic co-training framework designed to bridge the domain gap between 
                                    large-scale human videos and limited robot demonstrations. By leveraging DTW for human-to-robot mapping and MixUp-based interpolation 
                                    for domain adaptation, ImMimic enhances task performance across all embodiments.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        
            <!-- Video Processing Section -->
            <h6 class="pub-type-header">Video Processing</h6>
            <div class="main-pub-card-container">
                <div class="pub-card" data-year="2024">
                    <div class="row">
                        <div class="col-l col-xs-12 col-lg-3">
                            <div class="pub-img-wrapper">
                            <img src="data/projects/PSEU.png" width="100%" class="pub-img"/>
                            </div>
                        </div>
                        <div class="col-r col-xs-12 col-lg-9">
                            <div class="pub-card-body">
                                <h5 class="title">Bridge the Gap: From Weak to Full Supervision for Temporal Action Localization with PseudoFormer</h5>
                                <h6 class="authors">Ziyi Liu<sup>‚Ä†</sup>, <strong>Yangcen Liu</strong> (First Student Author, Primary Contributor)</h6>
                                <p class="info">
                                    <span class="conference">CVPR 2025</span> /
                                    <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Bridge_the_Gap_From_Weak_to_Full_Supervision_for_Temporal_CVPR_2025_paper.pdf">Paper</a><br>
                                    PseudoFormer is a two-branch framework for temporal action localization that bridges the gap between weakly- and fully-supervised learning. It generates high-quality
                                     pseudo labels via cross-branch fusion and leverages them to train a full supervision branch, achieving state-of-the-art performance on THUMOS14 and ActivityNet1.3.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
        
                <div class="pub-card">
                    <div class="row">
                        <div class="col-l col-xs-12 col-lg-3">
                            <div class="pub-img-wrapper">
                            <img src="data/projects/STAT.png" width="100%" class="pub-img"/>
                        </div>
                        </div>
                        <div class="col-r col-xs-12 col-lg-9">
                            <div class="pub-card-body">
                                <h5 class="title">STAT: Towards Generalizable Temporal Action Localization</h5>
                                <h6 class="authors"><strong>Yangcen Liu</strong>, Ziyi Liu<sup>‚Ä†</sup>, Yuanhao Zhai, Wen Li, David Doerman, Junsong Yuan</h6>
                                <p class="info">
                                    <span class="conference">Arxiv</span> /
                                    <a href="http://export.arxiv.org/abs/2404.13311">Paper</a><br>
                                    STAT is a self-supervised teacher-student framework for weakly-supervised temporal action localization, designed to improve 
                                    generalization across diverse distributions. It is able to adapt to varying 
                                    action durations, achieving strong cross-dataset performance on THUMOS14, ActivityNet1.2, and HACS.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
        
                <div class="pub-card">
                    <div class="row">
                        <div class="col-l col-xs-12 col-lg-3">
                            <div class="pub-img-wrapper">
                            <img src="data/projects/DAIR.png" width="100%" class="pub-img"/>
                            </div>
                        </div>
                        <div class="col-r col-xs-12 col-lg-9">
                            <div class="pub-card-body">
                                <h5 class="title">Simultaneous Detection and Interaction Reasoning for Object-Centric Action Recognition</h5>
                                <h6 class="authors">Xunsong Li, Pengzhan Sun, <strong>Yangcen Liu</strong>, Lixin Duan, Wen Li<sup>‚Ä†</sup></h6>
                                <p class="info">
                                    <span class="conference">TMM 2025</span> /
                                    <a href="https://ieeexplore.ieee.org/document/10891539">Paper</a><br>
                                    DAIR is an end-to-end framework for object-centric action recognition that jointly detects relevant objects 
                                    and reasons about their interactions. It integrates object decoding, interaction refinement, and relational 
                                    modeling without relying on a pre-trained detector, achieving strong results on Something-Else and IKEA-Assembly.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        
            <!-- AIGC Section -->
            <h6 class="pub-type-header">AIGC</h6>
            <div class="main-pub-card-container">
                <div class="pub-card" data-year="2023">
                    <div class="row">
                        <div class="col-l col-xs-12 col-lg-3">
                            <div class="pub-img-wrapper">
                            <img src="data/projects/AAGN.png" width="100%" class="pub-img"/>
                            </div>
                        </div>
                        <div class="col-r col-xs-12 col-lg-9">
                            <div class="pub-card-body">
                                <h5 class="title">Structure-Aware Human Body Reshaping with Adaptive Affinity-Graph Network</h5>
                                <h6 class="authors"><strong>Yangcen Liu</strong><sup>*</sup>, Qiwen Deng<sup>*</sup>, Wen Li, Guoqing Wang<sup>‚Ä†</sup></h6>
                                <p class="info">
                                    <span class="conference">WACV 2025</span> /
                                    <a href="https://ieeexplore.ieee.org/abstract/document/10943660">Paper</a><br>
                                    AAGN is a structure-aware framework for automatic human body reshaping in portraits that leverages an Adaptive Affinity-Graph 
                                    block to model global relationships between body parts and a Body Shape Discriminator guided by an SRM filter to ensure 
                                    high-frequency detail preservation. Trained on BR-5K, it generates coherent and aesthetically improved reshaping results.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        


        <div id="main-pub-container">
            <h2>Experience</h2>

            <div id="main-pub-card-container">

                <div class="pub-card" data-topic="experience" data-selected="False">
                    <div class="row">
                        <div class="col-l col-xs-12 col-lg-3">
                            <img src="data/gatech.png" width="100%" class="pub-img" playsinline autoplay muted loop preload/>
                        </div>
                        <div class="col-r col-xs-12 col-lg-9">
                            <div class="pub-card-body">
                                <h5 class="title">Research Assistant, Georgia Institute of Technology</h5>
                                Aug 2024 -- Now
                                <p class="info">
                                    Lab: <a href="https://faculty.cc.gatech.edu/~danfei/">Robot Learning and Reasoning Lab</a><br>
                                    Supervisor: <a href="https://faculty.cc.gatech.edu/~danfei/">Prof. Danfei Xu</a> </p>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="pub-card" data-topic="experience" data-selected="False">
                    <div class="row">
                        <div class="col-l col-xs-12 col-lg-3">
                            <img src="data/xiaohongshu.png" width="100%" class="pub-img" playsinline autoplay muted loop preload/>
                        </div>
                        <div class="col-r col-xs-12 col-lg-9">
                            <div class="pub-card-body">
                                <h5 class="title">AIGC Algorithm Researcher Intern, RED</h5>
                                Jul 2023 -- Sep 2023
                                <p class="info">
                                    <!-- communication platform for enterprises -->
                                    Worked for <a href="https://www.xiaohongshu.com/explore?language=en-US">Red Note (Xiaohongshu)</a>, is a social media and e-commerce platform with 300M+ active users (up to 2024)
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="pub-card" data-topic="experience" data-selected="False">
                    <div class="row">
                        <div class="col-l col-xs-12 col-lg-3">
                            <img src="data/buffalo.png" width="100%" class="pub-img" playsinline autoplay muted loop preload/>
                        </div>
                        <div class="col-r col-xs-12 col-lg-9">
                            <div class="pub-card-body">
                                <h5 class="title">Research Assistant, University at Buffalo (SUNY)</h5>
                                Mar 2023 -- Dec 2023 <!-- , Spring Semester -->
                                <p class="info">
                                    Lab: <a href="https://cse.buffalo.edu/~jsyuan/team.html">Visual Computing Lab</a><br>
                                    Supervisor: <a href="https://cse.buffalo.edu/~jsyuan/index.html">Prof. Junsong Yuan</a> &nbsp;
                                    Collaborated with <a href="https://scholar.google.com/citations?user=m2k89xgAAAAJ">Prof. Ziyi Liu</a>, <a
                                      href="https://www.yhzhai.com/">Yuanhao Zhai</a> and <a href="https://cse.buffalo.edu/~doermann/">Prof.
                                      David Doermann</a><br>
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="pub-card" data-topic="experience" data-selected="False">
                    <div class="row">
                        <div class="col-l col-xs-12 col-lg-3">
                            <img src="data/uestc_icon.png" width="100%" class="pub-img" playsinline autoplay muted loop preload/>
                        </div>
                        <div class="col-r col-xs-12 col-lg-9">
                            <div class="pub-card-body">
                                <h5 class="title">Research Assistant, University of Electronic Science and Technology of China</h5>
                                Mar 2022 -- Feb 2024 <!-- , Spring Semester -->
                                <p class="info">
                                    Lab: <a href="">Data Intelligence Group</a><br>
                                    Supervisor: <a href="https://wenli-vision.github.io/">Prof. Wen Li</a> &nbsp;
                                    Collaborated with <a href="https://scholar.google.com/citations?user=inRIcS0AAAAJ&hl=zh-CN">Prof. Lixin Duan</a>, <a
                                      href="https://shuhanggu.github.io/">Prof. Shuhang Gu</a> and Xunsong Li<br>
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

            </div>



                

        </div>

        <div id="main-pub-container">
            <h2>Teaching</h2>
        
            <div id="main-pub-card-container">

                <div class="pub-card" data-topic="teaching" data-selected="False">
                    <div class="row">
                        <div class="col-l col-xs-12 col-lg-3">
                            <img src="data/gtbee.jpg" width="100%" playsinline autoplay muted loop preload/>
                        </div>
                        <div class="col-r col-xs-12 col-lg-9">
                            <div class="pub-card-body">
                                <h5 class="title">Teaching Assistant, Georgia Institute of Technology</h5>
                                Aug 2025 -- Dec 2025
                                <p class="info">
                                    Course: <a href="https://sites.cc.gatech.edu/classes/AY2023/cs7643_fall/">Deep Learning (CS 4644/7643) Fall 2025</a><br>
                                    Supervisor: <a href="https://faculty.cc.gatech.edu/~danfei/">Prof. Danfei Xu</a>
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
        
                <div class="pub-card" data-topic="teaching" data-selected="False">
                    <div class="row">
                        <div class="col-l col-xs-12 col-lg-3">
                            <img src="data/gtbee.jpg" width="100%" playsinline autoplay muted loop preload/>
                        </div>
                        <div class="col-r col-xs-12 col-lg-9">
                            <div class="pub-card-body">
                                <h5 class="title">Teaching Assistant, Georgia Institute of Technology</h5>
                                Jan 2025 -- May 2025
                                <p class="info">
                                    Course: <a href="https://faculty.cc.gatech.edu/~zk15/teaching/AY2024_cs7643_spring/index.html">Deep Learning (OMSCS 4644/7643) Spring 2025</a><br>
                                    Supervisor: <a href="https://faculty.cc.gatech.edu/~zk15/">Prof. Kira Szolt</a>
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
        
            </div>
        </div>
        

        </div>

        <div style="height: 20px;"></div>


  <section id="reviewing-service" style="margin-top: 2em;">
  <h2>Service</h2>
  <p>
    CVPR 2026, ICRA 2026, CoRL 2025, MM 2024, MM 2023.
  </p>
</section>

    </main>
  </div>


        <section id="embodiment" class="container" style="max-width:1080px;padding-top:30px;">
  <div id="main-pub-container">
    <h2>Embodiment Experience</h2>

    <div class="embodiment-grid">
      <a href="https://franka.de/" target="_blank" style="text-decoration:none;color:inherit;">
        <div class="embodiment-card">
          <img src="data/robots/franka.png" class="embodiment-img" />
          <p class="embodiment-desc">Franka Emika Panda - A classic 7-DoF torque-controlled arm for precision manipulation.</p>
        </div>
      </a>

      <a href="https://www.kinovarobotics.com/product/gen3-robots" target="_blank" style="text-decoration:none;color:inherit;">
        <div class="embodiment-card">
          <img src="data/robots/Kinova.png" class="embodiment-img" />
          <p class="embodiment-desc">Kinova Gen3 - A modular and lightweight 7-DoF robotic arm with ROS-native control. Small but flexible.</p>
        </div>
      </a>

      <a href="https://www.rainbow-robotics.com/en_rby1" target="_blank" style="text-decoration:none;color:inherit;">
        <div class="embodiment-card">
          <img src="data/robots/rainbow.png" class="embodiment-img" />
          <p class="embodiment-desc">RB-Y1 - Wheeled dual-arm humanoid robot designed for agile mobile manipulation in diverse environments.</p>
        </div>
      </a>

      <a href="https://arx-x.com/" target="_blank" style="text-decoration:none;color:inherit;">
        <div class="embodiment-card">
          <img src="data/robots/arx.png" class="embodiment-img" />
          <p class="embodiment-desc">ARX - A bimanual setup, portable robotic arm designed for a wide range of applications.</p>
        </div>
      </a>

      <a href="https://robotiq.com/products/adaptive-grippers" target="_blank" style="text-decoration:none;color:inherit;">
        <div class="embodiment-card">
          <img src="data/robots/robotiq.png" class="embodiment-img" />
          <p class="embodiment-desc">Robotiq 2F-85 - A claasic parallel gripper with robust industrial design for versatile grasping.</p>
        </div>
      </a>

      <a href="https://source-robotics.com/blogs/blog/soft-robotic-grippers-fin-ray-effect?srsltid=AfmBOorHTLxJ4sm318jSHfdO6NPSzLnrEIQSmH2YCU1uoWYPrSN0IP-r" target="_blank" style="text-decoration:none;color:inherit;">
        <div class="embodiment-card">
          <img src="data/robots/finray.png" class="embodiment-img" />
          <p class="embodiment-desc">Fin Ray Gripper - A robust gripper. Compliant finger structure inspired by fish fins for enveloping grasps.</p>
        </div>
      </a>

      <a href="https://www.allegrohand.com/" target="_blank" style="text-decoration:none;color:inherit;">
        <div class="embodiment-card">
          <img src="data/robots/allegro.png" class="embodiment-img" />
          <p class="embodiment-desc">Allegro Hand - 16-DoF anthropomorphic hand for dexterous manipulation and learning.</p>
        </div>
      </a>

      <a href="https://www.psyonic.io/" target="_blank" style="text-decoration:none;color:inherit;">
        <div class="embodiment-card">
          <img src="data/robots/ability.png" class="embodiment-img" />
          <p class="embodiment-desc">Ability Hand - 6-DoF high-performance bionic hand designed for both prosthetics and research.</p>
        </div>
      </a>

      <a href="https://www.robotera.com/en/goods1/4.html" target="_blank" style="text-decoration:none;color:inherit;">
        <div class="embodiment-card">
          <img src="data/robots/xhand.png" class="embodiment-img" />
          <p class="embodiment-desc">XHAND - Five-finger robotic hand with 12-DoF, enabling dexterous manipulation and precise finger control.</p>
        </div>
      </a>
    </div>
  </div>
</section>



  <!--
  <section id="travel-map" class="container" style="max-width:950px;padding-top:30px;">
  <h5 class="subtitle">I shall tread this wondrous world!</h5>
  <iframe style="width:100%;height:480px;border:none;"
          src="https://www.google.com/maps/d/u/0/embed?mid=1Iik_qZzMt-zOv61n2a3TN7vaaYoEkPU&ll=1.3987233352870305%2C0&z=2">
  </iframe>
    </section>
    -->

</div>


